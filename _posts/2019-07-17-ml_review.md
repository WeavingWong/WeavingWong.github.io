---

layout: post

date: 2019-07-17 23:36:15.000000000 +09:00

title: 机器学习重要知识点回顾

categories: 理论基础

tags: 秋招加油

---



###  什么是bias和variance

> - 误差的期望值可以分解为三个部分：样本噪音、模型预测值的方差、预测值相对真实值的偏差。
> - 其中噪声是考虑到样本数据其实是采样，模型预测得到的y值其实叠加了部分误差。
> - 特定模型重复多次训练会得到多个具体的模型，每一个具体模型对测试样本进行一次预测，对于所有预测值的平均就是预测值的期望，该期望值与真实值之间的距离体现了模型偏差，而预测值的离散程度，即是模型的方差。
> - 评估方法：K-Fold 交叉验证
> #### 解决方案
>
> - 正则化项
> - 样本数量
>
>   模型方差较高时，增加样本会有帮助
>
>   模型偏差较高时，增加样本帮助不大）
>  - Bagging是Bootstrap Aggregating的简称，意思是再抽样。具体而言，当决策树不限制深度或不进行剪枝时，极容易出现过拟合。集成学习中采用bagging就是随机森林，通过对多个决策树取平均，可以减小过拟合，即降低方差。（用过过强的分类器，解决过拟合）
> - Boosting是将一个弱分类器的误差或者残差，作为下一个弱分类器的输入，通过弱分类器的叠加组合，可以降低偏差。（用于过弱的分类器，解决欠拟合问题）
>
> 参考：https://www.jianshu.com/p/8c7f033be58a

---

### k折交叉验证中k取值多少有什么关系

- 交叉验证的根本原因是数据集太小，而较小的K值会导致可用于建模的数据量太小，所以小数据集的交叉验证结果需要格外注意。建议选择较大的K值。

-  对于数据量较小的场景来说，留一法往往比K折更为有效。

- 从实验角度来看，较大的K值也不一定就能给出更小的方差，一切都需要具体情况具体讨论。相对而言，较大的K值的交叉验证结果倾向于更好。但同时也要考虑较大K值的计算开销。

- 2017年的一项研究给出了另一种经验式的选择方法，作者建议k=log(n)且保证n/k>3d，n代表了数据量，d代表了特征数。感兴趣的朋友可以对照论文进一步了解。

---

### 分类模型和回归模型的区别是什么

- 分类和回归的区别在于输出变量的类型
- 当然，这个类型的根本而在于损失函数的形式不同

定量输出称为回归，或者说是连续变量预测；
定性输出称为分类，或者说是离散变量预测。

分类和回归模型的本质是一样的，分类模型可以将回归模型的输出离散化，回归模型也可以将分类模型的输出连续化。

---

### 为什么会产生过拟合

- 产生的原因

（1）训练数据中噪音干扰过大（2）训练数据太少 （3）模型参数过多，模型复杂度高

---

###  如何解决过拟合

- 增大数据量
- 降低模型复杂度，使其适合自己训练集的数量级（缩小宽度和减小深度）
- 减少feature个数（人工定义留多少个feature或者算法选取这些feature）
- 正则化（留下所有的feature，但对于部分feature定义其parameter非常小
- dropout
- early stopping
- ensembel
- 交叉验证
- 重新清洗数据 

https://blog.csdn.net/NIGHT_SILENT/article/details/80795640

---

###  写出信息增益，信息增益比和基尼系数的公式

- ID3中的信息增益
	
	首先要了解什么是信息熵(Info Entropy)，信息熵是信息论中的一个重要物理量，高中学化学时候接触过熵的概念，用于描述物质的混乱程度，熵值越大，混乱程度越大，熵值越小，混乱程度越小，当熵值为0时，说明是纯物质，类比过来，就是一个确定信息，当熵值最大时，说明物质完全混乱，类比过来，就是一个完全不确定信息。我们应用决策树算法的目的，就是从众多属性中，通过不断选择属性，使得我们的决策分枝越来越明确，即信息熵越来越小。在选择属性的过程中，为了让算法更快收敛，我们按照属性对决策结果影响程度由打到小依次选取，这就需要定义一个物理量，用来描述属性对信息确定性的影响程度，这个物理量就是信息增益，其实就是选取某一属性信息熵的增量。
	

信息熵公式：
	
![](https://img-blog.csdn.net/20170805200400349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
	

	对于事件B，在事件A发生情况下的条件信息熵为：
	
	![img](https://img-blog.csdn.net/20170805200424189?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
	
	信息增益公式如下，即属性A的发生与否，对与事件B的影响增量
	
	![img](https://img-blog.csdn.net/20170805200443289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

- C4.5算法中的信息增益率

  信息增益率(Info Gain Ratio)也称信息增益比，用于解决信息增益对属性选择取值较多的问题，信息增益率为信息增益与该特征的信息熵之比。公式如下：

  ![img](https://img-blog.csdn.net/20170805200742723?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)


- CART算法中的基尼系数

  基尼指数(Gini Index)是CART算法的衡量依据。决策树的生成就是递归地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。

  其基本原理如下：假设某分类问题有K个类别，样本点属于第k类的概率为pk，则概率分布的基尼指数为：

  ![img](https://img-blog.csdn.net/20170805201521251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

  在属性A发生的情况下，决策事件B发生的基尼指数为：
  
  ![img](https://img-blog.csdn.net/20170805201623670?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9ndWVzaXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
  
  参考：https://blog.csdn.net/roguesir/article/details/76619919

### 写出LR的目标函数和损失函数

给定数据集![img](https://img-blog.csdnimg.cn/20190319101750342.png)其中（xi,yi）表示第i个样本，其中![img](https://img-blog.csdnimg.cn/20190319101843477.png)。即每个数据有n个特征，类别![img](https://img-blog.csdnimg.cn/20190319101858526.png)，要求训练数据，将数据分成两类0或1。

假定xi的n个特征为**线性关系**，即：

![img](https://img-blog.csdnimg.cn/20190319102139551.png)

这里为了表示简洁，在数据样本xi添加一个特征x0=1  将b作为![img](https://img-blog.csdnimg.cn/20190319102250703.png)。则有：

![img](https://img-blog.csdnimg.cn/20190319102640861.png)

可以使用阶跃函数，但是阶跃函数性质不好，不可导求解过于复杂，这里选用Sigmoid函数:


![img](https://img-blog.csdnimg.cn/20190319102843249.png)

![img](https://img-blog.csdnimg.cn/20190319102949283.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

当输入一个Z时，y输出一个0--1之间的数，假定y>0.5则最终结果判为1  y<0.5最终结果为0。当y=0.8时，最终结果为1,y=0.8也表征了此时输出为1的概率，令：

![img](https://img-blog.csdnimg.cn/20190319103139843.png)

将样本特征线性表示，然后输入到Sigmoid函数，输出结果在0--1之间，并且输出结果表征了分类结果为1的概率，即有：

![img](https://img-blog.csdnimg.cn/20190319103409283.png)

即![img](https://img-blog.csdnimg.cn/20190319103433461.png)输出刚好代表了结果为1的概率

**LR逻辑回归假设样本服从泊松0--1分布**，因此p(y|x)表达式：

![img](https://img-blog.csdnimg.cn/20190319103625185.png)

假设样本独立且同分布，最大似然估计：

![img](https://img-blog.csdnimg.cn/20190319103857311.png)

进而求最大对数似然估计：

![img](https://img-blog.csdnimg.cn/20190319104343686.png)

第一个问题，为什么要求最大对数似然估计而不是最大似然估计：

![img](https://img-blog.csdnimg.cn/20181102193421943.png)

第二个问题，LR的损失函数是什么：

![img](https://img-blog.csdnimg.cn/20190319104622683.png)

损失函数表征预测值与真实值之间的差异程度，如果预测值与真实值越接近则损失函数应该越小。在此损失函数可以取为最大似然估计函数的相反数，其次除以m这一因子并不改变最终求导极值结果，通过除以m可以得到平均损失值，避免样本数量对于损失值的影响。

这里采用随机梯度下降，损失函数对于![\theta](https://private.codecogs.com/gif.latex?%5Ctheta)j偏导：

![img](https://img-blog.csdnimg.cn/20190319111303902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMDY2NDQ=,size_16,color_FFFFFF,t_70)

![\theta](https://private.codecogs.com/gif.latex?%5Ctheta)j的迭代式：

![img](https://img-blog.csdnimg.cn/20190319111540468.png)



参考：https://blog.csdn.net/u014106644/article/details/83660226

---

## 常见的损失函数

机器学习或者统计机器学习常见的损失函数如下：

1.0-1损失函数 （0-1 loss function） 



L(Y,f(X))={1,0,Y ≠ f(X)Y = f(X)L(Y,f(X))={1,Y ≠ f(X)0,Y = f(X)



2.平方损失函数（quadratic loss function) 



L(Y,f(X))=(Y−f(x))2L(Y,f(X))=(Y−f(x))2



3.绝对值损失函数(absolute loss function) 



L(Y,f(x))=|Y−f(X)|L(Y,f(x))=|Y−f(X)|



4.对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) 



L(Y,P(Y|X))=−logP(Y|X)L(Y,P(Y|X))=−logP(Y|X)



逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好。

## 说说对数损失函数与平方损失函数

在逻辑回归的推导中国，我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值。整体的思想就是求极大似然函数的思想。而取对数，只是为了方便我们的在求MLE(Maximum Likelihood Estimation)过程中采取的一种数学手段而已。

## 损失函数详解

参考：https://blog.csdn.net/bitcarmanlee/article/details/51165444

---

### LR和SVM的异同

- 相同与联系

  1. 都是分类算法

  2. 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。

  3. LR和SVM都是监督学习算法。

  4. LR和SVM都是判别模型。

     - 常见的判别模型有：KNN、SVM、LR。 

     - 常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。

- 不同点

  1. loss function 不同 (重点)
  2. SVM 只考虑局部的边界线附近的点，LR 考虑全局，远离的点对边界线的确定也起作用
  3. 在解决非线性问题时，SVM 采用核函数的机制，而 LR 通常不采用核函数的方法
  4. 线性 SVM 依赖数据表达的距离测度，所以需要先对数据做 normalization, LR 则不受影响。
  5. SVM 的损失函数就自带正则，即1/2(∥w∥)^2, 这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！

- LR 和 SVM 的选择

  1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
  2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
  3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

- svm kernel 一般怎么选

  1. 如果数据量不是特别大的时候，可以使用 RBF kernel，主要要加入正则化； 
  2. 如果数据量很大，需要把特征离散化，拉高维度，使用 linear kernel 的 SVM

  参考1：https://www.cnblogs.com/zhizhan/p/5038747.html

  参考2：https://blog.csdn.net/kisslotus/article/details/80106191

  ---

### 推导SVM



---

### SVM在哪个地方引入核函数

对解线性分类问题，线性分类支持向量机开始一种非常有效的方法。但是，有事分类问题是非线性的，这时可以 使用非线性支持向量机。其基本想法是通过一个非线性变换将输入空间对应于一个特征空间，使得在输入空间中的超曲面模型对应于特征空间中的超平面。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以额完成。

---

### 常用的核函数有哪些，说说它们的特点及调参方式

- 多项式核函数（polynomial kernel function）

  ```
  image processing，参数比RBF多，取值范围是(0,inf)
  ```

- 高斯核函数（Gaussian kernel function）

  ```
  通用，线性不可分时，特征维数少 样本数量正常时，在没有先验知识时用，取值在[0,1]
  ```

- 线性核函数

  ```
  线性可分时，特征数量多时，样本数量多再补充一些特征时，linear kernel可以是RBF kernel的特殊情况
  ```

- 在 sklearn 中可以用 grid search 找到合适的 kernel，以及它们的 gamma，C 等参数

  其中有两个重要的参数，即 C（惩罚系数） 和 gamma，

  gamma 越大，支持向量越少，gamma 越小，支持向量越多。

  而支持向量的个数影响训练和预测的速度。

  C 越高，容易过拟合。C 越小，容易欠拟合。

---

### 随机森林的学习过程

简单来说，随机森林就是由多棵CART（Classification And Regression Tree）构成的。对于每棵树，它们使用的训练集是从总的训练集中有放回采样出来的，这意味着，总的训练集中的有些样本可能多次出现在一棵树的训练集中，也可能从未出现在一棵树的训练集中。在训练每棵树的节点时，使用的特征是从所有特征中按照一定比例随机地无放回的抽取的。

那随机森林具体如何构建呢？有两个方面：数据的随机性选取，以及待选特征的随机选取。

- 预测阶段

  如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。

  参考：https://blog.csdn.net/findsd1989/article/details/46388961

### 随机森林的优缺点

- 优点

  1.  在当前的很多数据集上，相对其他算法有着很大的优势，表现良好

  2. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择(  PS：特征子集是随机选择的)
  3. 在训练完后，它能够给出哪些feature比较重要( PS：http://blog.csdn.net/keepreder/article/details/47277517)
  4. 在创建随机森林的时候，对generlization error使用的是无偏估计，模型泛化能力强
  5. 训练速度快，容易做成并行化方法   PS：训练时树与树之间是相互独立的
  6. 在训练过程中，能够检测到feature间的互相影响
  7.  实现比较简单
  8. 对于不平衡的数据集来说，它可以平衡误差。
  9. 如果有很大一部分的特征遗失，仍可以维持准确度。

- 缺点：

  1. 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟

  2. 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。
  3. 相比于单一决策树，它的随机性让我们难以对模型进行解释。

  参考：https://blog.csdn.net/keepreder/article/details/47273297

  ---

### GBDT和随机森林的区别

1. 随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。

2. 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。

3. 组成随机森林的树可以并行生成；而GBDT只能是串行生成。

4. 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。

5. 随机森林对异常值不敏感；GBDT对异常值非常敏感。

6. 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。

7. 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。

   

参考：https://blog.csdn.net/login_sonata/article/details/73929426 

### DBDT和Xgboost 区别

- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
- xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

  参考：[http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/](http://wepon.me/2016/05/07/XGBoost浅入浅出/)

---

### 线性分类器与非线性分类器的区别及优劣

 - 常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归，SVM（线性核）等。

   线性分类器速度快、编程方便且便于理解，但是拟合能力低。

- 非线性分类器就是用一个“超曲面”或者多个超平（曲）面的组合将正、负样本隔离开（即，不属于线性的分类器），如： 
   （1）二维平面上的正、负样本用一条曲线或折线来进行分类； 
   （2）三维立体空间内的正、负样本用一个曲面或者折面来进行分类； 
   （3）N维空间内的正负样本用一个超曲面来进行分类。

- 常见的非线性分类器：决策树、RF、GBDT、多层感知机、SVM（高斯核）等。

   非线性分类器拟合能力强但是编程实现较复杂，理解难度大。

   参考：https://blog.csdn.net/weixin_42715356/article/details/82732207

   ---

### 特征比数据量还大时，选择什么样的分类器


- 特征比数据还多，意味着整体来看，数据是稀疏的，

  把特征视为维度，高维度下稀疏的数据很有可能是线性可分的，

  综上，选择线性分类器

- 非线性的目的是为了将低维度的数据映射到高维度空间，使其线性可分，现在特征比数据还大，意味着你的数据已经处于一个高维度的空间中了
- 或者可以尝试采用PCA降维

---

### L1和L2正则的区别，如何选择L1和L2正则

- L0范数与L1范数

    L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0即让参数W是稀疏的。 
    L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，|w|在w=0处是不可微， 
    为什么L0和L1都可以实现稀疏，但常用的为L1？因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。 

    综上，L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 

    ![img](https://img-blog.csdn.net/20180625101454939?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05JR0hUX1NJTEVOVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

    那么参数稀疏有什么好处呢？这里扯两点：

    （1）特征选择(Feature Selection)： 
    大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 

    （2）可解释性(Interpretability)： 
    另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。

- L2范数
  
    在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”(weight decay)。 weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。

    ![image](https://img-blog.csdn.net/20180624233541575?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05JR0hUX1NJTEVOVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

- dropout

dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。即在训练时候以一定的概率p来跳过一定的神经元。
    
![img](https://img-blog.csdn.net/20161107214940967?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

参考: https://blog.csdn.net/NIGHT_SILENT/article/details/80795640
    
---

### 多分类怎么处理

**多分类预测方案的对比：**

- 在预测中可以将多分类问题看做多个二分类问题（One-Vs-All），选择其中一个类别为正类（Positive），使其他所有类别为负类（Negative）。比如第一步，我们可以将三角形所代表的实例全部视为正类，其他实例全部视为负类。在预测阶段，每个分类器可以根据测试样本，得到当前正类的概率。即 P(y = i | x; θ)，i = 1, 2, 3..6。选择计算结果最高的分类器，其正类就可以作为预测结果。

-  相比于 One-Vs-All 由于样本数量可能的偏向性带来的不稳定性，One-Vs-One 是一种相对稳健的扩展方法。对于3分类问题，我们像举行车轮作战一样让不同类别的数据两两组合训练分类器，可以得到3 个二元分类器。任何一个测试样本都可以通过分类器的投票选举出预测结果，这就是 One-Vs-One 的运行方式。（缺点是类别比较多时，需要训练处更多的classifier,影响预测时间）

- Softmax：在二元逻辑回归中，我们用 Sigmoid 函数将一个多维数据（一个样本）映射到一个 0 - 1 之间的数值上，那有没有什么方法从数学上让一个样本映射到多个 0 - 1 之间的数值呢？有！我们可以通过 Softmax 函数，使所有概率之和为 1，是对概率分布进行归一化。在处理一些样本可能丛属多个类别的分类问题是，使用 one vs one 或 one vs all 有可能达到更好的效果。Softmax 回归适合处理一个样本尽可能属于一种类别的多分类问题。

###  如何进行特征选择

- 使用pearson 相关系数（缺点是支队线性关系敏感）

- 使用互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)

- 距离相关系数 (Distance correlation)

- 基于学习模型的特征排序 (Model based ranking)（lgb中可以是split,或是gain）

- 线性模型和正则化

参考：https://blog.csdn.net/keepreder/article/details/47278785

---



###  模型复杂度怎么表示？

- 支持向量机用L2范数来表示模型的复杂度。
- 原始的逻辑斯蒂回归与最大熵模型没有正则化，可以给他们加上L2正则化项。
- 提升方法没有显示的正则化项，通常通过早停（early stopping）的方法达到正则化的效果。

---

### 各种模型的特点

- 直接学习条件概率分布p(Y|X)或者决策函数f(x)的方法为判别方法，对应判别模型（感知机、K近邻、决策树、逻辑斯蒂回归于最大熵模型、支持向量机、提升方法、条件随机场）

- 首先学习联合概率分布从而求得天剑概率分布的P（Y|X）的方法是生成方法。（朴素贝叶斯、隐马尔可夫）

---

### 各种最优化方法比较梯度下降、拟牛顿法和牛顿法区别，哪个收敛快？为什么？ 

---

### 的优化方法有哪些？ sgd、adam、adgrad区别？ adagrad详细说一下？为什么adagrad适合处理稀疏梯度？

---

### DL常用的激活函数有哪些？ 

---

### relu和sigmoid有什么区别，优点有哪些？ 

---



### DNN的初始化方法有哪些？ 为什么要做初始化？ kaiming初始化方法的过程是怎样的？ 

---

### xgboost在什么地方做的剪枝，怎么做的？ 

---

### xgboost如何分布式？特征分布式和数据分布式？ 各有什么存在的问题？ 



### lightgbm和xgboost有什么区别？他们的loss一样么？ 算法层面有什么区别？ 

---

### lightgbm有哪些实现，各有什么区别？ 

---